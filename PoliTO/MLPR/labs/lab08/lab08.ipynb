{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import scipy.optimize as opt\n",
    "import sklearn.datasets as datasets\n",
    "from dataclasses import dataclass\n",
    "from typing import Literal, Generator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def vcol(vec: npt.NDArray) -> npt.NDArray:\n",
    "    return vec.reshape(-1, 1)\n",
    "\n",
    "\n",
    "def vrow(vec: npt.NDArray) -> npt.NDArray:\n",
    "    return vec.reshape(1, -1)\n",
    "\n",
    "def split_db_2to1(\n",
    "    D: npt.NDArray, L: npt.NDArray, seed=0\n",
    ") -> tuple[tuple[npt.NDArray, npt.NDArray], tuple[npt.NDArray, npt.NDArray]]:\n",
    "    nTrain = int(D.shape[1] * 2.0 / 3.0)\n",
    "    np.random.seed(seed)\n",
    "    idx = np.random.permutation(D.shape[1])\n",
    "    idxTrain = idx[0:nTrain]\n",
    "    idxTest = idx[nTrain:]\n",
    "\n",
    "    DTR = D[:, idxTrain]\n",
    "    DTE = D[:, idxTest]\n",
    "    LTR = L[idxTrain]\n",
    "    LTE = L[idxTest]\n",
    "\n",
    "    return (DTR, LTR), (DTE, LTE)\n",
    "\n",
    "\n",
    "def confusion_matrix(y_true: npt.NDArray, y_pred: npt.NDArray) -> npt.NDArray[np.int32]:\n",
    "    \"\"\"Compute the confusion matrix for the given true and predicted labels\n",
    "\n",
    "    Args:\n",
    "        y_true (npt.NDArray): The true labels\n",
    "        y_pred (npt.NDArray): The predicted labels\n",
    "\n",
    "    Returns:\n",
    "        npt.NDArray: The confusion matrix with the following structure:\n",
    "            [[TN, FP],\n",
    "             [FN, TP]]\n",
    "    \"\"\"\n",
    "    return np.array(\n",
    "        [\n",
    "            [\n",
    "                np.sum(np.logical_and(y_true == 0, y_pred == 0)),\n",
    "                np.sum(np.logical_and(y_true == 0, y_pred == 1)),\n",
    "            ],\n",
    "            [\n",
    "                np.sum(np.logical_and(y_true == 1, y_pred == 0)),\n",
    "                np.sum(np.logical_and(y_true == 1, y_pred == 1)),\n",
    "            ],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def yield_confusion_matrices(\n",
    "    y_true: npt.NDArray, thresholds: npt.NDArray\n",
    ") -> Generator[npt.NDArray[np.int32], None, None]:\n",
    "    indices = np.argsort(thresholds)\n",
    "    ts = thresholds[indices]\n",
    "    sorted_y_val = y_true[indices]\n",
    "\n",
    "    y_pred = np.ones_like(y_true)\n",
    "\n",
    "    TN = 0\n",
    "    TP = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n",
    "    FP = len(y_true) - TP\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(1, len(ts)):\n",
    "        y_pred[i - 1] = 0\n",
    "\n",
    "        if sorted_y_val[i - 1] == 1:\n",
    "            TP -= 1\n",
    "            FN += 1\n",
    "        else:\n",
    "            FP -= 1\n",
    "            TN += 1\n",
    "\n",
    "        yield np.array([[TN, FP], [FN, TP]])\n",
    "\n",
    "\n",
    "def optimal_bayes_threshold(pi: float, C_fn: float, C_fp: float) -> float:\n",
    "    return -np.log((pi * C_fn) / ((1 - pi) * C_fp))\n",
    "\n",
    "\n",
    "def effective_prior(pi_T: float, C_fn: float, C_fp: float) -> float:\n",
    "    return (pi_T * C_fn) / (pi_T * C_fn + (1 - pi_T) * C_fp)\n",
    "\n",
    "\n",
    "def dcf(\n",
    "    llr: npt.NDArray,\n",
    "    y_val: npt.NDArray,\n",
    "    pi: float,\n",
    "    Cf_n: float,\n",
    "    Cf_p: float,\n",
    "    strategy: Literal[\"optimal\"] | Literal[\"min\"] | Literal[\"manual\"],\n",
    "    normalize=False,\n",
    "    threshold=0.0,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute the Detection Cost Function (DCF) for the given data and priors.\n",
    "\n",
    "    Args:\n",
    "        llr (NDArray): The log-likelihood ratio values.\n",
    "        y_val (NDArray): The true labels.\n",
    "        pi (float): The prior probability of a genuine sample.\n",
    "        Cf_n (float): The cost of false negative.\n",
    "        Cf_p (float): The cost of false positive.\n",
    "        strategy (\n",
    "            Literal[\"optimal\"]\n",
    "            | Literal[\"min\"]\n",
    "            | Literal[\"manual\"],\n",
    "        ): The threshold strategy to use, either \"optimal\", \"min\", or \"manual\".\n",
    "            Use \"optimal\" to compute the optimal threshold, \"min\" to compute the\n",
    "            minimum DCF value, and \"manual\" to use the given threshold.\n",
    "        normalize (bool, optional): Whether to normalize the DCF value.\n",
    "            Defaults to False.\n",
    "        threshold (float, optional): The threshold to use if strategy is \"manual\".\n",
    "            Does not have any effect if strategy is not \"manual\". Defaults to 0.0.\n",
    "\n",
    "    Returns:\n",
    "        float: The DCF value.\n",
    "    \"\"\"\n",
    "\n",
    "    if strategy == \"min\":\n",
    "        # Returns the minimum DCF value calculated over all the possible\n",
    "        # threhsolds (taken from the log likelihood ratios)\n",
    "        return min(\n",
    "            [\n",
    "                (\n",
    "                    pi * (cm[1, 0] / cm[1].sum()) * Cf_n\n",
    "                    + (1 - pi) * (cm[0, 1] / cm[0].sum()) * Cf_p\n",
    "                )\n",
    "                / (min(pi * Cf_n, (1 - pi) * Cf_p) if normalize else 1)\n",
    "                for cm in yield_confusion_matrices(y_val, llr)\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        threshold = (\n",
    "            optimal_bayes_threshold(pi, Cf_n, Cf_p)\n",
    "            if strategy == \"optimal\"\n",
    "            else threshold  # if strategy == \"manual\"\n",
    "        )\n",
    "\n",
    "        y_pred = llr > threshold\n",
    "\n",
    "        cm = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "        P_fn = cm[1, 0] / cm[1].sum()\n",
    "        P_fp = cm[0, 1] / cm[0].sum()\n",
    "\n",
    "        return (pi * P_fn * Cf_n + (1 - pi) * P_fp * Cf_p) / (\n",
    "            # Normalize the DCF value by dividing it by the best of the two\n",
    "            # dummy systems: the one that always accepts a test segment and\n",
    "            # the one that always rejects it.\n",
    "            min(pi * Cf_n, (1 - pi) * Cf_p)\n",
    "            if normalize\n",
    "            else 1  # If normalization is not required, return the raw DCF value\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmin_l_bfgs_b\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mfprime\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mapprox_grad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mbounds\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mfactr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000000.0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mpgtol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-05\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1e-08\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0miprint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmaxfun\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmaxiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m15000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mdisp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mmaxls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Minimize a function func using the L-BFGS-B algorithm.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "func : callable f(x,*args)\n",
      "    Function to minimize.\n",
      "x0 : ndarray\n",
      "    Initial guess.\n",
      "fprime : callable fprime(x,*args), optional\n",
      "    The gradient of `func`. If None, then `func` returns the function\n",
      "    value and the gradient (``f, g = func(x, *args)``), unless\n",
      "    `approx_grad` is True in which case `func` returns only ``f``.\n",
      "args : sequence, optional\n",
      "    Arguments to pass to `func` and `fprime`.\n",
      "approx_grad : bool, optional\n",
      "    Whether to approximate the gradient numerically (in which case\n",
      "    `func` returns only the function value).\n",
      "bounds : list, optional\n",
      "    ``(min, max)`` pairs for each element in ``x``, defining\n",
      "    the bounds on that parameter. Use None or +-inf for one of ``min`` or\n",
      "    ``max`` when there is no bound in that direction.\n",
      "m : int, optional\n",
      "    The maximum number of variable metric corrections\n",
      "    used to define the limited memory matrix. (The limited memory BFGS\n",
      "    method does not store the full hessian but uses this many terms in an\n",
      "    approximation to it.)\n",
      "factr : float, optional\n",
      "    The iteration stops when\n",
      "    ``(f^k - f^{k+1})/max{|f^k|,|f^{k+1}|,1} <= factr * eps``,\n",
      "    where ``eps`` is the machine precision, which is automatically\n",
      "    generated by the code. Typical values for `factr` are: 1e12 for\n",
      "    low accuracy; 1e7 for moderate accuracy; 10.0 for extremely\n",
      "    high accuracy. See Notes for relationship to `ftol`, which is exposed\n",
      "    (instead of `factr`) by the `scipy.optimize.minimize` interface to\n",
      "    L-BFGS-B.\n",
      "pgtol : float, optional\n",
      "    The iteration will stop when\n",
      "    ``max{|proj g_i | i = 1, ..., n} <= pgtol``\n",
      "    where ``proj g_i`` is the i-th component of the projected gradient.\n",
      "epsilon : float, optional\n",
      "    Step size used when `approx_grad` is True, for numerically\n",
      "    calculating the gradient\n",
      "iprint : int, optional\n",
      "    Controls the frequency of output. ``iprint < 0`` means no output;\n",
      "    ``iprint = 0``    print only one line at the last iteration;\n",
      "    ``0 < iprint < 99`` print also f and ``|proj g|`` every iprint iterations;\n",
      "    ``iprint = 99``   print details of every iteration except n-vectors;\n",
      "    ``iprint = 100``  print also the changes of active set and final x;\n",
      "    ``iprint > 100``  print details of every iteration including x and g.\n",
      "disp : int, optional\n",
      "    If zero, then no output. If a positive number, then this over-rides\n",
      "    `iprint` (i.e., `iprint` gets the value of `disp`).\n",
      "maxfun : int, optional\n",
      "    Maximum number of function evaluations. Note that this function\n",
      "    may violate the limit because of evaluating gradients by numerical\n",
      "    differentiation.\n",
      "maxiter : int, optional\n",
      "    Maximum number of iterations.\n",
      "callback : callable, optional\n",
      "    Called after each iteration, as ``callback(xk)``, where ``xk`` is the\n",
      "    current parameter vector.\n",
      "maxls : int, optional\n",
      "    Maximum number of line search steps (per iteration). Default is 20.\n",
      "\n",
      "Returns\n",
      "-------\n",
      "x : array_like\n",
      "    Estimated position of the minimum.\n",
      "f : float\n",
      "    Value of `func` at the minimum.\n",
      "d : dict\n",
      "    Information dictionary.\n",
      "\n",
      "    * d['warnflag'] is\n",
      "\n",
      "      - 0 if converged,\n",
      "      - 1 if too many function evaluations or too many iterations,\n",
      "      - 2 if stopped for another reason, given in d['task']\n",
      "\n",
      "    * d['grad'] is the gradient at the minimum (should be 0 ish)\n",
      "    * d['funcalls'] is the number of function calls made.\n",
      "    * d['nit'] is the number of iterations.\n",
      "\n",
      "See also\n",
      "--------\n",
      "minimize: Interface to minimization algorithms for multivariate\n",
      "    functions. See the 'L-BFGS-B' `method` in particular. Note that the\n",
      "    `ftol` option is made available via that interface, while `factr` is\n",
      "    provided via this interface, where `factr` is the factor multiplying\n",
      "    the default machine floating-point precision to arrive at `ftol`:\n",
      "    ``ftol = factr * numpy.finfo(float).eps``.\n",
      "\n",
      "Notes\n",
      "-----\n",
      "License of L-BFGS-B (FORTRAN code):\n",
      "\n",
      "The version included here (in fortran code) is 3.0\n",
      "(released April 25, 2011). It was written by Ciyou Zhu, Richard Byrd,\n",
      "and Jorge Nocedal <nocedal@ece.nwu.edu>. It carries the following\n",
      "condition for use:\n",
      "\n",
      "This software is freely available, but we expect that all publications\n",
      "describing work using this software, or all commercial products using it,\n",
      "quote at least one of the references given below. This software is released\n",
      "under the BSD License.\n",
      "\n",
      "References\n",
      "----------\n",
      "* R. H. Byrd, P. Lu and J. Nocedal. A Limited Memory Algorithm for Bound\n",
      "  Constrained Optimization, (1995), SIAM Journal on Scientific and\n",
      "  Statistical Computing, 16, 5, pp. 1190-1208.\n",
      "* C. Zhu, R. H. Byrd and J. Nocedal. L-BFGS-B: Algorithm 778: L-BFGS-B,\n",
      "  FORTRAN routines for large scale bound constrained optimization (1997),\n",
      "  ACM Transactions on Mathematical Software, 23, 4, pp. 550 - 560.\n",
      "* J.L. Morales and J. Nocedal. L-BFGS-B: Remark on Algorithm 778: L-BFGS-B,\n",
      "  FORTRAN routines for large scale bound constrained optimization (2011),\n",
      "  ACM Transactions on Mathematical Software, 38, 1.\n",
      "\n",
      "Examples\n",
      "--------\n",
      "Solve a linear regression problem via `fmin_l_bfgs_b`. To do this, first we define\n",
      "an objective function ``f(m, b) = (y - y_model)**2``, where `y` describes the\n",
      "observations and `y_model` the prediction of the linear model as\n",
      "``y_model = m*x + b``. The bounds for the parameters, ``m`` and ``b``, are arbitrarily\n",
      "chosen as ``(0,5)`` and ``(5,10)`` for this example.\n",
      "\n",
      ">>> import numpy as np\n",
      ">>> from scipy.optimize import fmin_l_bfgs_b\n",
      ">>> X = np.arange(0, 10, 1)\n",
      ">>> M = 2\n",
      ">>> B = 3\n",
      ">>> Y = M * X + B\n",
      ">>> def func(parameters, *args):\n",
      "...     x = args[0]\n",
      "...     y = args[1]\n",
      "...     m, b = parameters\n",
      "...     y_model = m*x + b\n",
      "...     error = sum(np.power((y - y_model), 2))\n",
      "...     return error\n",
      "\n",
      ">>> initial_values = np.array([0.0, 1.0])\n",
      "\n",
      ">>> x_opt, f_opt, info = fmin_l_bfgs_b(func, x0=initial_values, args=(X, Y),\n",
      "...                                    approx_grad=True)\n",
      ">>> x_opt, f_opt\n",
      "array([1.99999999, 3.00000006]), 1.7746231151323805e-14  # may vary\n",
      "\n",
      "The optimized parameters in ``x_opt`` agree with the ground truth parameters\n",
      "``m`` and ``b``. Next, let us perform a bound contrained optimization using the `bounds`\n",
      "parameter. \n",
      "\n",
      ">>> bounds = [(0, 5), (5, 10)]\n",
      ">>> x_opt, f_op, info = fmin_l_bfgs_b(func, x0=initial_values, args=(X, Y),\n",
      "...                                   approx_grad=True, bounds=bounds)\n",
      ">>> x_opt, f_opt\n",
      "array([1.65990508, 5.31649385]), 15.721334516453945  # may vary    \n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\occhi\\scoop\\apps\\miniconda3\\current\\envs\\ml-and-pr\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "opt.fmin_l_bfgs_b?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x: npt.ArrayLike) -> float:\n",
    "    y, z = x # x.shape = (2,)\n",
    "    return (y + 3)**2 + np.sin(y) + (z + 1)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call the function with `approx_grad=True` to use numerical gradient approximation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, f, d = opt.fmin_l_bfgs_b(f, np.array([0, 0]), approx_grad=True, iprint=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.57747138 -0.99999927] -0.3561430123647649 {'grad': array([-1.49324998e-06,  1.46549439e-06]), 'task': 'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL', 'funcalls': 21, 'nit': 6, 'warnflag': 0}\n"
     ]
    }
   ],
   "source": [
    "print(x, f, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute manually the gradient and return it as a tuple. Doing so leads to less function evaluations and faster convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x: npt.ArrayLike) -> float:\n",
    "    y, z = x # x.shape = (2,)\n",
    "    return ((y + 3)**2 + np.sin(y) + (z + 1)**2, np.array([2*(y + 3) + np.cos(y), 2*(z + 1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, f, d = opt.fmin_l_bfgs_b(f, np.array([0, 0]), approx_grad=False, iprint=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.57747137 -0.99999927] -0.3561430123647611 {'grad': array([-1.50318729e-06,  1.46120529e-06]), 'task': 'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL', 'funcalls': 7, 'nit': 6, 'warnflag': 0}\n"
     ]
    }
   ],
   "source": [
    "print(x, f, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_db_2to1(D, L, seed=0):\n",
    "    nTrain = int(D.shape[1] * 2.0 / 3.0)\n",
    "    np.random.seed(seed)\n",
    "    idx = np.random.permutation(D.shape[1])\n",
    "    idxTrain = idx[0:nTrain]\n",
    "    idxTest = idx[nTrain:]\n",
    "    DTR = D[:, idxTrain]\n",
    "    DTE = D[:, idxTest]\n",
    "    LTR = L[idxTrain]\n",
    "    LTE = L[idxTest]\n",
    "    return (DTR, LTR), (DTE, LTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_iris_binary():\n",
    "    D, L = datasets.load_iris()['data'].T, datasets.load_iris()['target']\n",
    "    D = D[:, L != 0] # We remove setosa from D\n",
    "    L = L[L!=0] # We remove setosa from L\n",
    "    L[L==2] = 0 # We assign label 0 to virginica (was label 2)\n",
    "    return D, L\n",
    "\n",
    "D, L = load_iris_binary()\n",
    "(DTR, LTR), (DVAL, LVAL) = split_db_2to1(D, L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Logistic Regression objective function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LogReg:\n",
    "    DTR : npt.NDArray[np.float64]\n",
    "    LTR : npt.NDArray[np.int64]\n",
    "    l : float\n",
    "    \n",
    "    def __call__(self, v: npt.ArrayLike) -> tuple[float, npt.NDArray[np.float64]]:\n",
    "        w, b = v[:-1], v[-1] # v.shape = (DTR.shape[0] + 1,)\n",
    "        ZTR = 2 * self.LTR - 1\n",
    "        S = (vcol(w).T @ self.DTR + b).ravel()\n",
    "        G = -ZTR / (1 + np.exp(ZTR * S))\n",
    "\n",
    "        return (\n",
    "            self.l / 2 * np.linalg.norm(w)**2 + np.mean(np.logaddexp(0, -ZTR * S))#,\n",
    "            # np.array([\n",
    "            #     *(self.l * w + np.mean(vrow(G) * DTR)),\n",
    "            #     np.mean(G)\n",
    "            # ])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "logRegObj = LogReg(DTR, LTR, 1.0)\n",
    "\n",
    "x, f, d = opt.fmin_l_bfgs_b(logRegObj, np.zeros(DTR.shape[0] + 1), approx_grad=True, iprint=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.11040207 -0.02898688 -0.24787109 -0.14950472  2.31094484] 0.6316436205354684 {'grad': array([-4.22994973e-06,  1.46216372e-05, -6.55031585e-07,  7.30526751e-06,\n",
      "        4.44089213e-07]), 'task': 'CONVERGENCE: REL_REDUCTION_OF_F_<=_FACTR*EPSMCH', 'funcalls': 132, 'nit': 19, 'warnflag': 0}\n"
     ]
    }
   ],
   "source": [
    "print(x, f, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRIOR = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = x[:-1], x[-1]\n",
    "\n",
    "S = vcol(w).T @ DVAL + b\n",
    "\n",
    "LP = S > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Error Rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1470588235294118"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - np.mean(LP == LVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1111111111111111 1.0\n"
     ]
    }
   ],
   "source": [
    "S_llr = S - np.log(PRIOR / (1 - PRIOR))\n",
    "\n",
    "S_llr = S_llr.T.ravel()\n",
    "\n",
    "min_dcf = dcf(S_llr, LVAL, 0.8, 1, 1, \"min\", normalize=True)\n",
    "act_dcf = dcf(S_llr, LVAL, 0.8, 1, 1, \"optimal\", normalize=True)\n",
    "\n",
    "print(min_dcf, act_dcf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying different $\\lambda$ values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.19685808 -0.02124461 -1.10336583 -0.80719171  8.17551683] 0.45394068959591793 {'grad': array([1.10467191e-06, 1.16018306e-06, 1.78190797e-06, 1.88737913e-07,\n",
      "       4.77395861e-07]), 'task': 'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL', 'funcalls': 174, 'nit': 24, 'warnflag': 0}\n",
      "0.11764705882352944\n"
     ]
    }
   ],
   "source": [
    "x, f, d = opt.fmin_l_bfgs_b(LogReg(DTR, LTR, 1e-1), np.zeros(DTR.shape[0] + 1), approx_grad=True, iprint=1)\n",
    "print(x, f, d)\n",
    "\n",
    "w, b = x[:-1], x[-1]\n",
    "S = vcol(w).T @ DVAL + b\n",
    "LP = S > 0\n",
    "print(1 - np.mean(LP == LVAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05555555555555555 0.7222222222222222\n"
     ]
    }
   ],
   "source": [
    "S_llr = S - np.log(PRIOR / (1 - PRIOR))\n",
    "\n",
    "S_llr = S_llr.T.ravel()\n",
    "\n",
    "min_dcf = dcf(S_llr, LVAL, 0.8, 1, 1, \"min\", normalize=True)\n",
    "act_dcf = dcf(S_llr, LVAL, 0.8, 1, 1, \"optimal\", normalize=True)\n",
    "\n",
    "print(min_dcf, act_dcf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.72973631  0.98290467 -4.54960463 -7.12478539 20.95261877] 0.11000090355092179 {'grad': array([8.42381725e-07, 4.42701429e-07, 4.85722576e-07, 1.33226764e-07,\n",
      "       2.77555733e-09]), 'task': 'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL', 'funcalls': 258, 'nit': 36, 'warnflag': 0}\n",
      "0.08823529411764708\n"
     ]
    }
   ],
   "source": [
    "x, f, d = opt.fmin_l_bfgs_b(LogReg(DTR, LTR, 1e-3), np.zeros(DTR.shape[0] + 1), approx_grad=True, iprint=1)\n",
    "print(x, f, d)\n",
    "\n",
    "w, b = x[:-1], x[-1]\n",
    "S = vcol(w).T @ DVAL + b\n",
    "LP = S > 0\n",
    "print(1 - np.mean(LP == LVAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1111111111111111 0.2222222222222222\n"
     ]
    }
   ],
   "source": [
    "S_llr = S - np.log(PRIOR / (1 - PRIOR))\n",
    "\n",
    "S_llr = S_llr.T.ravel()\n",
    "\n",
    "min_dcf = dcf(S_llr, LVAL, 0.8, 1, 1, \"min\", normalize=True)\n",
    "act_dcf = dcf(S_llr, LVAL, 0.8, 1, 1, \"optimal\", normalize=True)\n",
    "\n",
    "print(min_dcf, act_dcf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior-weighted logistic regression and calibration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LogRegWeighted:\n",
    "    DTR : npt.NDArray[np.float64]\n",
    "    LTR : npt.NDArray[np.int64]\n",
    "    l : float\n",
    "    pi : float\n",
    "    \n",
    "    def __call__(self, v: npt.ArrayLike) -> tuple[float, npt.NDArray[np.float64]]:\n",
    "        w, b = v[:-1], v[-1]\n",
    "        \n",
    "        ZTR = 2 * self.LTR - 1\n",
    "        S = (vcol(w).T @ self.DTR + b).ravel()\n",
    "        \n",
    "        n_T = np.sum(self.LTR == 1)\n",
    "        n_F = np.sum(self.LTR == 0)\n",
    "        \n",
    "        weights = np.where(self.LTR == 1, self.pi / n_T, (1 - self.pi) / n_F)\n",
    "\n",
    "        return self.l / 2 * np.linalg.norm(w)**2 + np.sum(weights * np.logaddexp(0, -ZTR * S))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.11036541 -0.0290465  -0.24798317 -0.14963204  2.25695248] 0.632017629678834 {'grad': array([-9.35918010e-06, -3.84137166e-06, -5.02931030e-06, -9.65894032e-07,\n",
      "       -2.25375275e-06]), 'task': 'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL', 'funcalls': 138, 'nit': 19, 'warnflag': 0}\n",
      "0.08823529411764708\n",
      "0.1111111111111111 0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "x, f, d = opt.fmin_l_bfgs_b(LogRegWeighted(DTR, LTR, 1.0, 0.5), np.zeros(DTR.shape[0] + 1), approx_grad=True, iprint=1)\n",
    "\n",
    "print(x, f, d)\n",
    "\n",
    "PRIOR = 0.8\n",
    "\n",
    "w, b = x[:-1], x[-1]\n",
    "\n",
    "S = vcol(w).T @ DVAL + b\n",
    "\n",
    "LP = S > 0\n",
    "\n",
    "print(1 - np.mean(LP == LVAL))\n",
    "\n",
    "S_llr = S - np.log(PRIOR / (1 - PRIOR))\n",
    "\n",
    "S_llr = S_llr.T.ravel()\n",
    "\n",
    "min_dcf = dcf(S_llr, LVAL, 0.8, 1, 1, \"min\", normalize=True)\n",
    "act_dcf = dcf(S_llr, LVAL, 0.8, 1, 1, \"optimal\", normalize=True)\n",
    "\n",
    "print(min_dcf, act_dcf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.25343795 -0.04809752 -0.94125105 -0.64605667  8.58673869] 0.36062613082254713 {'grad': array([1.84852134e-06, 6.16173779e-07, 1.52100554e-06, 6.82787157e-07,\n",
      "       1.99840128e-07]), 'task': 'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL', 'funcalls': 174, 'nit': 25, 'warnflag': 0}\n",
      "0.38235294117647056\n",
      "0.05555555555555555 0.7222222222222222\n"
     ]
    }
   ],
   "source": [
    "x, f, d = opt.fmin_l_bfgs_b(LogRegWeighted(DTR, LTR, 1e-1, 0.8), np.zeros(DTR.shape[0] + 1), approx_grad=True, iprint=1)\n",
    "print(x, f, d)\n",
    "\n",
    "PRIOR = 0.8\n",
    "\n",
    "w, b = x[:-1], x[-1]\n",
    "\n",
    "S = vcol(w).T @ DVAL + b\n",
    "\n",
    "LP = S > 0\n",
    "\n",
    "print(1 - np.mean(LP == LVAL))\n",
    "\n",
    "S_llr = S - np.log(PRIOR / (1 - PRIOR))\n",
    "\n",
    "S_llr = S_llr.T.ravel()\n",
    "\n",
    "min_dcf = dcf(S_llr, LVAL, 0.8, 1, 1, \"min\", normalize=True)\n",
    "act_dcf = dcf(S_llr, LVAL, 0.8, 1, 1, \"optimal\", normalize=True)\n",
    "\n",
    "print(min_dcf, act_dcf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.51852491  0.65126024 -4.3227396  -6.46160908 21.91047562] 0.09401035003010494 {'grad': array([-4.92245137e-06, -2.37310170e-06, -4.55330221e-06, -1.32810430e-06,\n",
      "       -8.36830536e-07]), 'task': 'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL', 'funcalls': 324, 'nit': 46, 'warnflag': 0}\n",
      "0.11764705882352944\n",
      "0.16666666666666666 0.2222222222222222\n"
     ]
    }
   ],
   "source": [
    "x, f, d = opt.fmin_l_bfgs_b(LogRegWeighted(DTR, LTR, 1e-3, 0.8), np.zeros(DTR.shape[0] + 1), approx_grad=True, iprint=1)\n",
    "print(x, f, d)\n",
    "\n",
    "PRIOR = 0.8\n",
    "\n",
    "w, b = x[:-1], x[-1]\n",
    "\n",
    "S = vcol(w).T @ DVAL + b\n",
    "\n",
    "LP = S > 0\n",
    "\n",
    "print(1 - np.mean(LP == LVAL))\n",
    "\n",
    "S_llr = S - np.log(PRIOR / (1 - PRIOR))\n",
    "\n",
    "S_llr = S_llr.T.ravel()\n",
    "\n",
    "min_dcf = dcf(S_llr, LVAL, 0.8, 1, 1, \"min\", normalize=True)\n",
    "act_dcf = dcf(S_llr, LVAL, 0.8, 1, 1, \"optimal\", normalize=True)\n",
    "\n",
    "print(min_dcf, act_dcf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-and-pr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
