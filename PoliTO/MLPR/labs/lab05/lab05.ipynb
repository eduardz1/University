{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as skdata\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_iris():\n",
    "    D, L = skdata.load_iris()[\"data\"].T, skdata.load_iris()[\"target\"]\n",
    "    return D, L\n",
    "\n",
    "\n",
    "def split_db_2to1(D, L, seed=0):\n",
    "    nTrain = int(D.shape[1] * 2.0 / 3.0)\n",
    "    np.random.seed(seed)\n",
    "    idx = np.random.permutation(D.shape[1])\n",
    "    idxTrain = idx[0:nTrain]\n",
    "    idxTest = idx[nTrain:]\n",
    "    DTR = D[:, idxTrain]\n",
    "    DTE = D[:, idxTest]\n",
    "    LTR = L[idxTrain]\n",
    "    LTE = L[idxTest]\n",
    "    return (DTR, LTR), (DTE, LTE)\n",
    "\n",
    "\n",
    "def logpdf_GAU_ND(X, mu, C):\n",
    "    X = np.atleast_2d(X)\n",
    "    mu = np.atleast_1d(mu)\n",
    "    C = np.atleast_2d(C)\n",
    "\n",
    "    return -0.5 * (\n",
    "        X.shape[0] * np.log(2 * np.pi)\n",
    "        + np.linalg.slogdet(C)[1]\n",
    "        + np.einsum(\"ij,ji->i\", np.dot((X - mu).T, np.linalg.inv(C)), (X - mu))\n",
    "    )\n",
    "\n",
    "\n",
    "def log_likelihood_GAU_ND(X, mu, C):\n",
    "    return np.sum(logpdf_GAU_ND(X, mu, C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all the testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "llr_MVG = np.load(\"solutions/llr_MVG.npy\")\n",
    "logMarginal_MVG = np.load(\"solutions/logMarginal_MVG.npy\")\n",
    "logMarginal_NaiveBayes = np.load(\"solutions/logMarginal_NaiveBayes.npy\")\n",
    "logMarginal_TiedMVG = np.load(\"solutions/logMarginal_TiedMVG.npy\")\n",
    "logPosterior_MVG = np.load(\"solutions/logPosterior_MVG.npy\")\n",
    "logPosterior_NaiveBayes = np.load(\"solutions/logPosterior_NaiveBayes.npy\")\n",
    "logPosterior_TiedMVG = np.load(\"solutions/logPosterior_TiedMVG.npy\")\n",
    "logSJoint_MVG = np.load(\"solutions/logSJoint_MVG.npy\")\n",
    "logSJoint_NaiveBayes = np.load(\"solutions/logSJoint_NaiveBayes.npy\")\n",
    "logSJoint_TiedMVG = np.load(\"solutions/logSJoint_TiedMVG.npy\")\n",
    "Posterior_MVG = np.load(\"solutions/Posterior_MVG.npy\")\n",
    "Posterior_NaiveBayes = np.load(\"solutions/Posterior_NaiveBayes.npy\")\n",
    "Posterior_TiedMVG = np.load(\"solutions/Posterior_TiedMVG.npy\")\n",
    "SJoint_MVG = np.load(\"solutions/SJoint_MVG.npy\")\n",
    "SJoint_NaiveBayes = np.load(\"solutions/SJoint_NaiveBayes.npy\")\n",
    "SJoint_TiedMVG = np.load(\"solutions/SJoint_TiedMVG.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, L = load_iris()\n",
    "# DTR and LTR are training data and labels, DTE and LTE are evaluation data and labels\n",
    "(DTR, LTR), (DTE, LTE) = split_db_2to1(D, L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Gaussian Classifier\n",
    "\n",
    "We compute the ML estimates for the classifier parameters $(\\mu_0, \\Sigma_0), (\\mu_1, \\Sigma_1), (\\mu_2, \\Sigma_2)$ using the training data. We then compute the likelihoods:\n",
    "\n",
    "$f_{\\boldsymbol{X} \\mid C}\\left(\\boldsymbol{x}_t \\mid c\\right)=\\mathcal{N}\\left(\\boldsymbol{x}_t \\mid \\boldsymbol{\\mu}_c^*, \\boldsymbol{\\Sigma}_c^*\\right)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 50)\n",
      "Class 0: mu = [[4.96129032]\n",
      " [3.42903226]\n",
      " [1.46451613]\n",
      " [0.2483871 ]], C = [[0.13140479 0.11370447 0.02862643 0.01187305]\n",
      " [0.11370447 0.16270552 0.01844953 0.01117586]\n",
      " [0.02862643 0.01844953 0.03583767 0.00526535]\n",
      " [0.01187305 0.01117586 0.00526535 0.0108845 ]]\n",
      "Likelihoods of class 0 = [4.75727828e+000 3.12730541e+000 8.18872691e-062 6.74418077e-182\n",
      " 4.25115977e-209 1.11347892e+001 8.20663072e+000 5.33350172e+000\n",
      " 5.65483426e-075 4.14843994e-073 2.02642087e+000 3.77771958e-001\n",
      " 9.83889710e-067 7.82296707e-001 1.46007373e-245 4.78978412e-034\n",
      " 1.84384639e-183 1.67069032e-072 4.56056472e-001 4.42425541e-106\n",
      " 1.72117487e-002 8.08671263e-213 8.79017787e+000 6.13458968e+000\n",
      " 1.33960789e-172 1.07167476e+001 1.80360838e-176 5.20856446e-061\n",
      " 3.67983764e-052 9.13121973e-037 1.77931741e-155 5.64070816e-152\n",
      " 1.69675933e-099 1.26208799e-107 2.40102284e+000 3.49099515e-043\n",
      " 7.08090459e-176 3.20056873e-127 1.08532794e+001 1.51993171e-077\n",
      " 2.16083562e-060 1.00671519e-105 3.56156641e-076 2.27780042e-001\n",
      " 4.92487274e+000 3.15819965e+000 3.71452449e-141 6.06520927e-053\n",
      " 3.16498493e-212 9.29266297e+000]\n",
      "Class 1: mu = [[5.91212121]\n",
      " [2.78484848]\n",
      " [4.27272727]\n",
      " [1.33939394]], C = [[0.26470156 0.09169881 0.18366391 0.05134068]\n",
      " [0.09169881 0.10613407 0.08898072 0.04211203]\n",
      " [0.18366391 0.08898072 0.21955923 0.06289256]\n",
      " [0.05134068 0.04211203 0.06289256 0.03208448]]\n",
      "Likelihoods of class 1 = [1.82592287e-23 5.86395481e-33 8.73291496e-01 2.62818969e-09\n",
      " 1.84804537e-08 1.35834983e-32 8.87841907e-25 3.76912576e-31\n",
      " 1.53338448e+00 1.26263597e+00 1.16374152e-28 8.19533011e-20\n",
      " 2.07668692e+00 5.06869370e-26 4.15270442e-17 4.60606743e-01\n",
      " 1.51137563e-04 6.22022783e-01 6.88829381e-49 2.50142880e-02\n",
      " 2.18224092e-47 1.69090052e-09 1.46890801e-34 1.66726218e-25\n",
      " 6.27234108e-06 8.27554000e-29 2.08232939e-11 5.32500670e+00\n",
      " 3.51466418e+00 2.91944072e-01 1.67601023e-10 8.66750244e-13\n",
      " 1.77573569e-01 2.83450633e-01 4.77615848e-23 9.66652683e-01\n",
      " 2.19576855e-13 1.81348901e-05 3.59906246e-30 1.20023337e+00\n",
      " 1.97667051e+00 4.48417131e-02 2.68315844e-01 5.50403973e-35\n",
      " 7.77744502e-30 8.20142505e-28 1.70755948e-03 9.58812972e-02\n",
      " 2.37977460e-06 2.66930802e-26]\n",
      "Class 2: mu = [[6.45555556]\n",
      " [2.92777778]\n",
      " [5.41944444]\n",
      " [1.98888889]], C = [[0.30080247 0.08262346 0.18614198 0.04311728]\n",
      " [0.08262346 0.08533951 0.06279321 0.05114198]\n",
      " [0.18614198 0.06279321 0.18434414 0.04188272]\n",
      " [0.04311728 0.05114198 0.04188272 0.0804321 ]]\n",
      "Likelihoods of class 2 = [7.75729447e-44 5.42775880e-57 1.87025065e-03 1.17277225e+00\n",
      " 3.06406819e-02 1.73784702e-55 6.98574960e-45 2.96041534e-54\n",
      " 7.77031792e-03 2.24511040e-05 3.93574192e-53 7.69561673e-42\n",
      " 2.38770689e-05 6.33818592e-51 4.51507678e-06 1.21455281e-07\n",
      " 1.35166349e-03 3.06757212e-06 4.24807017e-70 3.85478956e-01\n",
      " 5.60760195e-77 2.84245862e-03 2.42910512e-58 1.16417666e-43\n",
      " 1.79524265e-01 6.43284358e-51 2.69940777e-01 2.47311128e-04\n",
      " 9.05811806e-06 4.10084995e-09 1.12516413e+00 3.72516956e-01\n",
      " 2.67316608e-01 1.27531036e-01 3.63960882e-42 2.76830108e-06\n",
      " 1.00392713e+00 1.44362882e+00 1.46301585e-51 2.92669481e-04\n",
      " 2.44254173e-05 5.69306467e-02 9.35414972e-04 8.13301173e-61\n",
      " 1.57893054e-54 2.44896381e-46 1.13323793e+00 9.33462845e-06\n",
      " 2.11385175e-03 1.35355141e-46]\n"
     ]
    }
   ],
   "source": [
    "classes = np.unique(LTE)\n",
    "S = np.zeros((len(classes), DTE.shape[1]))\n",
    "\n",
    "print(DTE.shape)\n",
    "\n",
    "for i, label in enumerate(classes):\n",
    "    mu = np.mean(DTR[:, LTR == label], axis=1, keepdims=True)\n",
    "    C = np.cov(DTR[:, LTR == label], bias=True)\n",
    "    print(f\"Class {i}: mu = {mu}, C = {C}\")\n",
    "\n",
    "    log_pdf = logpdf_GAU_ND(DTE, mu, C)\n",
    "\n",
    "    likelihood = np.exp(log_pdf)\n",
    "\n",
    "    print(f\"Likelihoods of class {i} = {likelihood}\")\n",
    "\n",
    "    S[i, :] = likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix of Joint Densities\n",
    "\n",
    "$f_{\\boldsymbol{X}, C}\\left(\\boldsymbol{x}_t, c\\right)=f_{\\boldsymbol{X} \\mid C}\\left(\\boldsymbol{x}_t \\mid c\\right) P_C(c)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.75727828e+000 3.12730541e+000 8.18872691e-062 6.74418077e-182\n",
      " 4.25115977e-209 1.11347892e+001 8.20663072e+000 5.33350172e+000\n",
      " 5.65483426e-075 4.14843994e-073 2.02642087e+000 3.77771958e-001\n",
      " 9.83889710e-067 7.82296707e-001 1.46007373e-245 4.78978412e-034\n",
      " 1.84384639e-183 1.67069032e-072 4.56056472e-001 4.42425541e-106\n",
      " 1.72117487e-002 8.08671263e-213 8.79017787e+000 6.13458968e+000\n",
      " 1.33960789e-172 1.07167476e+001 1.80360838e-176 5.20856446e-061\n",
      " 3.67983764e-052 9.13121973e-037 1.77931741e-155 5.64070816e-152\n",
      " 1.69675933e-099 1.26208799e-107 2.40102284e+000 3.49099515e-043\n",
      " 7.08090459e-176 3.20056873e-127 1.08532794e+001 1.51993171e-077\n",
      " 2.16083562e-060 1.00671519e-105 3.56156641e-076 2.27780042e-001\n",
      " 4.92487274e+000 3.15819965e+000 3.71452449e-141 6.06520927e-053\n",
      " 3.16498493e-212 9.29266297e+000]\n",
      "--------------\n",
      "[1.58575943e+000 1.04243514e+000 2.72957564e-062 2.24806026e-182\n",
      " 1.41705326e-209 3.71159641e+000 2.73554357e+000 1.77783391e+000\n",
      " 1.88494475e-075 1.38281331e-073 6.75473622e-001 1.25923986e-001\n",
      " 3.27963237e-067 2.60765569e-001 4.86691243e-246 1.59659471e-034\n",
      " 6.14615463e-184 5.56896774e-073 1.52018824e-001 1.47475180e-106\n",
      " 5.73724955e-003 2.69557088e-213 2.93005929e+000 2.04486323e+000\n",
      " 4.46535964e-173 3.57224919e+000 6.01202795e-177 1.73618815e-061\n",
      " 1.22661255e-052 3.04373991e-037 5.93105804e-156 1.88023605e-152\n",
      " 5.65586443e-100 4.20695998e-108 8.00340945e-001 1.16366505e-043\n",
      " 2.36030153e-176 1.06685624e-127 3.61775980e+000 5.06643905e-078\n",
      " 7.20278540e-061 3.35571731e-106 1.18718880e-076 7.59266808e-002\n",
      " 1.64162425e+000 1.05273322e+000 1.23817483e-141 2.02173642e-053\n",
      " 1.05499498e-212 3.09755432e+000]\n",
      "--------------\n",
      "[1.58575943e+000 1.04243514e+000 2.72957564e-062 2.24806026e-182\n",
      " 1.41705326e-209 3.71159641e+000 2.73554357e+000 1.77783391e+000\n",
      " 1.88494475e-075 1.38281331e-073 6.75473622e-001 1.25923986e-001\n",
      " 3.27963237e-067 2.60765569e-001 4.86691243e-246 1.59659471e-034\n",
      " 6.14615463e-184 5.56896774e-073 1.52018824e-001 1.47475180e-106\n",
      " 5.73724955e-003 2.69557088e-213 2.93005929e+000 2.04486323e+000\n",
      " 4.46535964e-173 3.57224919e+000 6.01202795e-177 1.73618815e-061\n",
      " 1.22661255e-052 3.04373991e-037 5.93105804e-156 1.88023605e-152\n",
      " 5.65586443e-100 4.20695998e-108 8.00340945e-001 1.16366505e-043\n",
      " 2.36030153e-176 1.06685624e-127 3.61775980e+000 5.06643905e-078\n",
      " 7.20278540e-061 3.35571731e-106 1.18718880e-076 7.59266808e-002\n",
      " 1.64162425e+000 1.05273322e+000 1.23817483e-141 2.02173642e-053\n",
      " 1.05499498e-212 3.09755432e+000]\n"
     ]
    }
   ],
   "source": [
    "# Multiply each row by the prior (1/3)\n",
    "SJoint = S * 1 / 3\n",
    "\n",
    "assert np.allclose(SJoint, SJoint_MVG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Posterior Probabilities\n",
    "\n",
    "$P\\left(C=c \\mid \\boldsymbol{X}=\\boldsymbol{x}_t\\right)=\\frac{f_{\\boldsymbol{X}, C}\\left(\\boldsymbol{x}_t, c\\right)}{\\sum_{c^{\\prime}} f_{\\boldsymbol{X}, C}\\left(\\boldsymbol{x}_{\\boldsymbol{t}}, c^{\\prime}\\right)}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMarginal = np.sum(SJoint, axis=0)\n",
    "SPost = SJoint / SMarginal\n",
    "\n",
    "assert np.allclose(SPost, Posterior_MVG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 0 1 2 2 0 0 0 1 1 0 0 1 0 2 1 2 1 0 2 0 2 0 0 2 0 2 1 1 1 2 2 2 1 0 1 2\n",
      " 2 0 1 1 2 1 0 0 0 2 1 2 0]\n"
     ]
    }
   ],
   "source": [
    "predictions = np.argmax(SPost, axis=0)\n",
    "\n",
    "print(f\"Predictions: {predictions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "acc = np.mean(predictions == LTE)\n",
    "\n",
    "print(f\"Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate: 0.040000000000000036\n"
     ]
    }
   ],
   "source": [
    "err = 1 - acc\n",
    "\n",
    "print(f\"Error rate: {err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint Log Density\n",
    "\n",
    "$l_c=\\log f_{\\boldsymbol{X}, C}\\left(\\boldsymbol{x}_t, c\\right)=\\log f_{\\boldsymbol{X} \\mid C}\\left(\\boldsymbol{x}_t \\mid c\\right)+\\log P_C(c)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "logSJoint = np.log(SJoint)\n",
    "\n",
    "assert np.allclose(logSJoint, logSJoint_MVG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Marginal Density\n",
    "\n",
    "### log-sum-exp trick\n",
    "\n",
    "$\\log f_{\\boldsymbol{X}}\\left(\\boldsymbol{x}_t\\right)=\\log \\sum_c e^{l_c}$\n",
    "\n",
    "Computing the exponential might result in numerical errors, so we rewrite the sum as :\n",
    "\n",
    "$\\log \\sum_c e^{l_c}=l+\\log \\sum_c e^{l_c-l}$\n",
    "\n",
    "where $l=\\max_c l_c$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "logSMarginal = scipy.special.logsumexp(logSJoint, axis=0)\n",
    "\n",
    "assert np.allclose(logSMarginal, logMarginal_MVG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Posterior\n",
    "\n",
    "$\\log P\\left(C=c \\mid \\boldsymbol{X}=\\boldsymbol{x}_t\\right)=\\log f_{\\boldsymbol{X}, C}\\left(\\boldsymbol{x}_t, c\\right)-\\log f_{\\boldsymbol{X}}\\left(\\boldsymbol{x}_t\\right)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "logSPost = logSJoint - logSMarginal\n",
    "\n",
    "assert np.allclose(logSPost, logPosterior_MVG)\n",
    "\n",
    "SPost = np.exp(logSPost)\n",
    "\n",
    "assert np.allclose(SPost, Posterior_MVG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Gaussian Classifier\n",
    "\n",
    "The Naive Bayes version of the Multi Variate Gaussian is simply a Gaussian classifier where the covariance matrices are diagonal. The ML solution for the covariance matrices, therefore, is:\n",
    "\n",
    "$\\operatorname{diag}\\left(\\boldsymbol{\\Sigma}_c^*\\right)=\\operatorname{diag}\\left[\\frac{1}{N_c} \\sum_i\\left(\\boldsymbol{x}_{c, i}-\\boldsymbol{\\mu}_c^*\\right)\\left(\\boldsymbol{x}_{c, i}-\\boldsymbol{\\mu}_c^*\\right)^T\\right]$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96\n",
      "Error rate: 0.040000000000000036\n"
     ]
    }
   ],
   "source": [
    "for i, label in enumerate(classes):\n",
    "    mu = np.mean(DTR[:, LTR == label], axis=1, keepdims=True)\n",
    "    C = np.cov(DTR[:, LTR == label], bias=True)\n",
    "\n",
    "    # Make the covariance matrix diagonal\n",
    "    C = np.diag(np.diag(C))\n",
    "\n",
    "    log_pdf = logpdf_GAU_ND(DTE, mu, C)\n",
    "    likelihood = np.exp(log_pdf)\n",
    "    S[i, :] = likelihood\n",
    "\n",
    "# Multiply each row by the prior (1/3)\n",
    "SJoint = S * 1 / 3\n",
    "\n",
    "assert np.allclose(SJoint, SJoint_NaiveBayes)\n",
    "\n",
    "# Compute the marginal likelihood\n",
    "SMarginal = np.sum(SJoint, axis=0)\n",
    "\n",
    "assert np.allclose(SMarginal, np.exp(logMarginal_NaiveBayes))\n",
    "\n",
    "# Compute the posterior\n",
    "SPost = SJoint / SMarginal\n",
    "\n",
    "assert np.allclose(SPost, Posterior_NaiveBayes)\n",
    "\n",
    "# Compute the predictions\n",
    "predictions = np.argmax(SPost, axis=0)\n",
    "accuracy = np.mean(predictions == LTE)\n",
    "error_rate = 1 - accuracy\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Error rate: {error_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tied Covariance Gaussian Classifier\n",
    "\n",
    "Similar to the previous classifier, the only ML estimate that changes is the covariance matrix. The ML estimate for the covariance matrix is:\n",
    "\n",
    "$\\boldsymbol{\\Sigma}^*=\\frac{1}{N} \\sum_c \\sum_i\\left(\\boldsymbol{x}_{c, i}-\\boldsymbol{\\mu}_c^*\\right)\\left(\\boldsymbol{x}_{c, i}-\\boldsymbol{\\mu}_c^*\\right)^T$\n",
    "\n",
    "(We notice that this is the within class scatter matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98\n",
      "Error rate: 0.020000000000000018\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([len(DTR[0, LTR == c]) for c in classes])\n",
    "\n",
    "Sw = np.average(\n",
    "    [np.atleast_2d(np.cov(DTR[:, LTR == c], bias=True)) for c in classes],\n",
    "    axis=0,\n",
    "    weights=weights,\n",
    ")\n",
    "\n",
    "for i, label in enumerate(classes):\n",
    "    D = DTR[:, LTR == label]\n",
    "    mu = np.mean(D, axis=1, keepdims=True)\n",
    "\n",
    "    log_pdf = logpdf_GAU_ND(DTE, mu, Sw)\n",
    "    likelihood = np.exp(log_pdf)\n",
    "    S[i, :] = likelihood\n",
    "\n",
    "# Multiply each row by the prior (1/3)\n",
    "SJoint = S * 1 / 3\n",
    "\n",
    "assert np.allclose(SJoint, SJoint_TiedMVG)\n",
    "\n",
    "# Compute the marginal likelihood\n",
    "SMarginal = np.sum(SJoint, axis=0)\n",
    "\n",
    "assert np.allclose(SMarginal, np.exp(logMarginal_TiedMVG))\n",
    "\n",
    "# Compute the posterior\n",
    "SPost = SJoint / SMarginal\n",
    "\n",
    "assert np.allclose(SPost, Posterior_TiedMVG)\n",
    "\n",
    "# Compute the predictions\n",
    "predictions = np.argmax(SPost, axis=0)\n",
    "accuracy = np.mean(predictions == LTE)\n",
    "error_rate = 1 - accuracy\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Error rate: {error_rate}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary tasks: log-likelihood Ratios and MVG\n",
    "\n",
    "We can proceed in the same way but for binary tasks we can cast the classification as a comparison of a score, the _log-likelihood ratio_ with a threshold _t_ that depends on class priors.\n",
    "\n",
    "Assuming that class 2 is the _true_ class and class 1 is the _false_ class, the log-likelihood ratio is:\n",
    "\n",
    "$s(x_t) = llr(x_t) = \\log \\frac{f_{\\boldsymbol{X}|C}(x_t | 2)}{f_{\\boldsymbol{X}|C}(x_t | 1)} = \\log \\frac{\\mathcal{N}(x_t | {\\mu}_2, {\\Sigma}_2)}{\\mathcal{N}(x_t | {\\mu}_1, {\\Sigma}_1)} = \\log \\mathcal{N}(x_t | {\\mu}_2, {\\Sigma}_2) - \\log \\mathcal{N}(x_t | {\\mu}_1, {\\Sigma}_1)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restrict the dataset to class 1 and 2 (Versicolor and Virginica)\n",
    "\n",
    "DIris, LIris = load_iris()\n",
    "D = DIris[:, LIris != 0]\n",
    "L = LIris[LIris != 0]\n",
    "\n",
    "(DTR, LTR), (DVAL, LVAL) = split_db_2to1(D, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(LVAL)\n",
    "S = np.zeros((len(classes), DVAL.shape[1]))\n",
    "\n",
    "for i, label in enumerate(classes):\n",
    "    mu = np.mean(DTR[:, LTR == label], axis=1, keepdims=True)\n",
    "    C = np.cov(DTR[:, LTR == label], bias=True)\n",
    "\n",
    "    S[i, :] = logpdf_GAU_ND(DVAL, mu, C)\n",
    "\n",
    "# Compute the log-likelihood ratio\n",
    "llr = S[1] - S[0]\n",
    "\n",
    "assert np.allclose(llr, llr_MVG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threshold\n",
    "\n",
    "We assume uniform priors $P(C = 2) = P(C = 1) = 0.5$ so the threshold becomes `0`:\n",
    "\n",
    "$t = \\log \\frac{P(C = 2)}{P(C = 1)} = 0$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9117647058823529\n",
      "Error rate: 0.08823529411764708\n"
     ]
    }
   ],
   "source": [
    "predictions = llr > 0\n",
    "\n",
    "accuracy = np.mean(predictions == (LVAL == 2))\n",
    "error_rate = 1 - accuracy\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Error rate: {error_rate}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-and-pr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
